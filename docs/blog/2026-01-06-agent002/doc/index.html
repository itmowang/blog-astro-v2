<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="Astro description"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="generator" content="Astro v4.7.1"><!-- <link href="https://unpkg.com/aos@2.3.4/dist/aos.css" rel="stylesheet" /> --><!-- Canonical URL --><link rel="canonical" href="https://blog.loli.wang/blog/2026-01-06-agent002/doc/"><!-- Primary Meta Tags --><title>[学习] Token、Embedding 与向量空间 - 魔王の博客 </title><meta name="title" content="[学习] Token、Embedding 与向量空间 - 魔王の博客"><meta name="description" content="高兴的使用astro构建"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://blog.loli.wang/blog/2026-01-06-agent002/doc/"><meta property="og:title" content="[学习] Token、Embedding 与向量空间 - 魔王の博客"><meta property="og:description" content="高兴的使用astro构建"><meta property="og:image" content="https://blog.loli.wang/placeholder-social.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://blog.loli.wang/blog/2026-01-06-agent002/doc/"><meta property="twitter:title" content="[学习] Token、Embedding 与向量空间 - 魔王の博客"><meta property="twitter:description" content="高兴的使用astro构建"><meta property="twitter:image" content="https://blog.loli.wang/placeholder-social.jpg"><link rel="stylesheet" href="/_astro/hoisted.DH1pNdf-.css">
<link rel="stylesheet" href="/_astro/about.CrVYBgZN.css"><script type="module" src="/_astro/hoisted.CGSjG33K.js"></script></head> <body>  <div id="app" class="main"> <div class="sidebar"> <div class="top-container" data-aos="fade-right"> <div class="top-header-container"> <a href="https://blog.loli.wang/" title="Mowang - Your bio" class="site-title-container"><img src="https://avatars.githubusercontent.com/u/137391282?v=4" alt="Mowang" class="site-logo"> <h1 class="site-title">魔王の博客</h1></a> <div class="menu-btn"> <div class="line"></div> </div> </div> <div> <a href="/" title="Blog" class="site-nav"> Blog </a><a href="/life" title="Life" class="site-nav"> Life </a><a href="/archive" title="Archive" class="site-nav"> Archive </a><a href="/link" title="Link" class="site-nav"> Link </a><a href="/about" title="About" class="site-nav"> About </a><a href="https://github.com/itmowang" title="Github" class="site-nav"> Github </a> </div> </div> <div class="bottom-container" data-aos="flip-up" data-aos-offset="0"> <div class="social-container"></div> <div class="site-description">高兴的使用astro构建</div> <div class="site-footer"> © 2026 YOUR NAME HERE. | <a href="https://blog.loli.wang/rss.xml" title="RSS" target="_blank" class="rss">RSS</a> </div> </div> </div>  <div class="main-container"> <div class="content-container" data-aos="fade-up">  <div class="post-detail" id="lightgallery"> <h2 class="post-title">[学习] Token、Embedding 与向量空间</h2> <div class="post-date"> <time datetime="2026-01-06T15:27:24.000Z">2026-01-06 23:27</time> </div> <div class="post-share"> <div class="postShare"> <div> <div class="postShare-List">
分享本文 :
<a href="http://service.weibo.com/share/share.php?url=https://blog.loli.wang/blog/2026-01-06-agent002/doc/&title=undefined - 魔王の博客&pic=http://img.blog.loli.wang/2026-01-06-Agent002/01.png" target="_blank"> <i class="iconfont icon-xinlang" style="color:#ff763b"></i></a> <div class="wechat-container"> <a href="#"> <i id="wechatIcon" class="iconfont icon-weichat" style="color:#33b045"></i></a> <div class="wechat-dropdown" id="wechatDropdown"> <p>分享到微信</p> <div id="qrcode"></div> <p>扫描二维码</p> <p>可在微信查看或分享至朋友圈。</p> </div> </div> <a href="https://connect.qq.com/widget/shareqq/index.html?url=https://blog.loli.wang/blog/2026-01-06-agent002/doc/&title=undefined - 魔王の博客&source=undefined - 魔王の博客&desc=高兴的使用astro构建&pics=http://img.blog.loli.wang/2026-01-06-Agent002/01.png" target="_blank"> <i class="iconfont icon-QQ" style="color:#56b6e7"></i></a> <a href="" target="_blank"> <i class="iconfont icon-facebook" style="color:#44619D"></i></a> <a href="" target="_blank"> <i class="iconfont icon-fenxiang1" style="color:#33b045"></i></a> </div> </div> </div>  </div> <div class="feature-container" style="background-image: url('http://img.blog.loli.wang/2026-01-06-Agent002/01.png');"></div> <div class="post-content">  <h1 id="tokenembedding-与向量空间">Token、Embedding 与向量空间</h1>
<p>在上一节中讲到，LLM 的本质是<strong>一个在给定上下文条件下，预测下一个 token 概率分布的函数</strong>。那么 Token 到底是什么，为什么 LLM 要以 token 为单位，以及由此而来的 Embedding 和向量空间的概念。</p>
<h2 id="一什么是-token为什么-llm-的基本单位是-token">一、什么是 Token，为什么 LLM 的基本单位是 Token</h2>
<p>要真正理解 Token，必须回到计算机处理文字的底层逻辑。不能只停留在当下的 AI 浪潮中，而需要<strong>回到一个更根本的问题</strong>：<strong>计算机究竟是如何“看到”文字的？</strong></p>
<h3 id="1-追根溯源从二进制到字符">1. 追根溯源：从二进制到字符</h3>
<p>计算机的世界里，<strong>唯一真实存在的只有二进制</strong>。无论是文字、图片还是代码，最终都必须被表示为 0 和 1。因此，计算机并不能直接理解 <code>A</code>、<code>你</code>、<code>+</code> 等字符，它真正能处理的只有类似 <code>01000001</code> 这样的形式。</p>
<h3 id="2-编码encoding字符进入计算机的第一步">2. 编码（Encoding）：字符进入计算机的第一步</h3>
<p>为了让计算机“识别”人类的文字，出现了 <strong>编码（Encoding）</strong> 的概念——<strong>用数字去代表字符</strong>。最早被广泛使用的是 <strong>ASCII 编码</strong>：每个字符用 8 位二进制数表示，覆盖英文字符、数字和少量符号。例如：<code>A</code> → 十进制 <code>65</code>，<code>B</code> → 十进制 <code>66</code>。ASCII 解决了英文世界的问题，但它无法表示中文、日文等非拉丁字符。</p>
<h3 id="3-unicode让所有字符都能被编码">3. Unicode：让“所有字符”都能被编码</h3>
<p>为了解决多语言问题，<strong>Unicode</strong> 体系诞生了（常见实现如 UTF-8）。它的目标很明确：<strong>为世界上每一个字符分配一个唯一编号</strong>。在这一阶段，计算机已经可以显示中文、存储多语言文本并正确传输字符序列。</p>
<h3 id="4-token-的出现从编码走向统计压缩">4. Token 的出现：从“编码”走向“统计压缩”</h3>
<p>Token 的意义，正是在这里发生了质变。与 ASCII / Unicode 不同，<strong>Token 不再只是编码方案</strong>，而是一种基于语料统计的 <strong>压缩与建模方式</strong>：<strong>把经常一起出现的字符序列，打包成一个更高层级的单位</strong>。</p>
<p>换句话说：编码关心的是“这个字符用哪个数字表示？”Token 关心的是：“哪些字符经常一起出现，值得被当成一个整体？”</p>
<h3 id="5-一个直观对比同一句话在不同层级下的表示">5. 一个直观对比：同一句话在不同层级下的表示</h3>
<p>比如“人工智能”这几个字在编码和 Token 的不同表示：</p>

















<table><thead><tr><th align="left">层级</th><th align="left">表示方式</th></tr></thead><tbody><tr><td align="left">ASCII / Unicode</td><td align="left"><code>人</code> <code>工</code> <code>智</code> <code>能</code></td></tr><tr><td align="left">Token</td><td align="left"><code>人工</code> <code>智能</code></td></tr></tbody></table>
<p>可以看到，层级越低 → 越接近机器，序列越长；层级越高 → 越接近人类语义，结构越清晰。</p>
<h3 id="6-为什么-llm-的基本单位是-token">6. 为什么 LLM 的基本单位是 Token</h3>
<p>当前主流 LLM 的核心架构是 Transformer，而 Transformer 在底层本质上只支持 <strong>加法、乘法和矩阵运算</strong>。这意味着：<strong>任何进入模型的东西，必须先被表示为数字</strong>，模型无法直接理解或计算人类语言中的字符串、字符或词语。</p>
<p>因此，文本在进入模型之前，必须先被切分为 <strong>Token</strong>，再映射为对应的 <strong>Token ID</strong>，并进一步转换为向量表示（Embedding），才能参与注意力计算、矩阵运算以及梯度反向传播。在这一计算范式下，Token 不仅是一种文本表示方式，而是 Transformer <strong>唯一能够感知和操作的语言单位</strong>。模型所展现出的理解、推理与生成能力，全部涌现自 Token 级别的数值计算之上。</p>
<hr>
<h2 id="二embedding-是什么它解决了什么问题">二、Embedding 是什么，它解决了什么问题</h2>
<h3 id="1-embedding-是什么">1. Embedding 是什么</h3>
<p>在 LLM 中，Token 本身只是一个 <strong>离散 ID（整数）</strong>，例如 <code>12345</code>。这个数字对模型没有任何语义含义，ID 之间的大小、距离也毫无意义。Embedding 的作用，就是把这些离散、无序的 Token ID 映射到一个 <strong>连续的高维向量空间</strong> 中，使模型能够用数学方式刻画它们之间的<strong>相似性、方向性和组合关系</strong>。</p>
<p>在这个向量空间里，语义相近的 Token 会拥有相近的向量表示，不同语义则体现为不同的方向或距离，从而让注意力机制和矩阵运算“看见”语言结构与语义关系。简单来说，它是将离散的数字 ID 转化为一个<strong>高维连续向量</strong>（由数百或数千个实数组成的数组）的过程。</p>
<p>以“人工智能”为例，过程如下：</p>
<ol>
<li><strong>原始文本</strong>：“人工智能”</li>
<li><strong>Token 化</strong>：切分为 <code>[人工, 智能]</code></li>
<li><strong>Token ID</strong>：转换为离散数字 <code>[3421, 5678]</code></li>
<li><strong>Embedding 层</strong>：
<ul>
<li><code>3421</code> → <code>[0.12, -0.45, 0.88, ...]</code> (1536 个数字)</li>
<li><code>5678</code> → <code>[0.34, 0.11, -0.92, ...]</code> (1536 个数字)</li>
</ul>
</li>
</ol>
<h3 id="2-它解决了什么核心问题">2. 它解决了什么核心问题？</h3>
<p>在 Embedding 出现之前，计算机通过 <strong>One-hot Encoding（独热编码）</strong> 来处理文字。这种传统方式存在两个致命缺陷，而 Embedding 完美解决了它们：</p>
<h4 id="a-解决语义孤岛问题语义关联性">A. 解决“语义孤岛”问题（语义关联性）</h4>
<ul>
<li><strong>旧问题</strong>：在独热编码中，任何两个词的向量乘积都为 0。计算机认为“猫”和“狗”的距离，与“猫”和“手机”的距离是一样远的。它无法理解词语之间的含义联系。</li>
<li><strong>Embedding 方案</strong>：它将词语映射到一个<strong>语义空间</strong>。在这个空间里，意思相近的词（如“猫”和“小猫”）在几何距离上会靠得很近，而无关的词则很远。</li>
</ul>
<h4 id="b-解决维度灾难问题存储效率">B. 解决“维度灾难”问题（存储效率）</h4>
<ul>
<li><strong>旧问题</strong>：如果词表有 10 万个词，每个词都需要一个 10 万维的向量，且里面全是 0，极度浪费空间。</li>
<li><strong>Embedding 方案</strong>：它通过“降维打击”，用一个固定长度（如 GPT-3 的 12288 维）的<strong>稠密向量</strong>来表达。这个向量不仅省空间，还能承载极其丰富的语义细节。</li>
</ul>
<h3 id="3-embedding-的特性语义运算">3. Embedding 的特性：语义运算</h3>
<p>Embedding 最神奇的地方在于，它让语言具备了<strong>数学运算</strong>的可能性。在一个训练良好的 Embedding 空间中，你可以发现类似“类比”的逻辑关系。这说明模型已经“理解”了：<strong>女王之于女性，等同于国王之于男性。</strong> 这种性别、时态、甚至是逻辑上的关系，都被编码进了这些数字里。</p>
<p><em>(注：此处 Notebook 中包含一段 3Blue1Brown 的视频展示代码)</em></p>
<hr>
<h2 id="实战-生成-embedding">实战: 生成 Embedding</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sentence_transformers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> SentenceTransformer</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> pandas </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> pd</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 选择模型</span></span>
<span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SentenceTransformer(</span><span style="color:#9ECBFF">"all-MiniLM-L6-v2"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 生成embedding</span></span>
<span class="line"><span style="color:#E1E4E8">model.encode(</span><span style="color:#9ECBFF">'dog'</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>输出结果：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>array([-5.31470478e-02,  1.41944205e-02,  7.14570703e-03,  6.86086714e-02,</span></span>
<span class="line"><span>       -7.84803182e-02,  1.01674581e-02,  1.02283150e-01, -1.20648630e-02,</span></span>
<span class="line"><span>        ...</span></span>
<span class="line"><span>        1.11444503e-01,  2.98568588e-02,  2.39054970e-02,  1.10093102e-01],</span></span>
<span class="line"><span>      dtype=float32)</span></span>
<span class="line"><span></span></span></code></pre>
<hr>
<h2 id="三相似性在向量空间中如何体现">三、相似性在向量空间中如何体现</h2>
<p>LLM 里的“相似”是指<strong>向量方向相近</strong>。常用指标是 <strong>Cosine Similarity（余弦相似度）</strong>：</p>
<ul>
<li><code>cos(θ) → 1</code>：非常相似</li>
<li><code>cos(θ) → 0</code>：无关</li>
<li><code>cos(θ) → -1</code>：相反</li>
</ul>
<p><em>(注：此处 Notebook 中包含一段关于词向量相似度的视频展示代码)</em></p>
<hr>
<h2 id="实战-计算相似度">实战: 计算相似度</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> cosine_similarity</span><span style="color:#E1E4E8">(a, b):</span></span>
<span class="line"><span style="color:#E1E4E8">    a </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array(a)</span></span>
<span class="line"><span style="color:#E1E4E8">    b </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array(b)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> np.dot(a, b) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (np.linalg.norm(a) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> np.linalg.norm(b))</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">pairs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#9ECBFF">"北京"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"上海"</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#9ECBFF">"北京"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"东京"</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#9ECBFF">"北京"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"苹果"</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#9ECBFF">"Python"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"Java"</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#9ECBFF">"Python"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"香蕉"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> a, b </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> pairs:</span></span>
<span class="line"><span style="color:#E1E4E8">    sim </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cosine_similarity(model.encode(a), model.encode(b))</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">a</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> vs </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">b</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">sim</span><span style="color:#F97583">:.3f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>输出结果：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>北京 vs 上海: 0.378</span></span>
<span class="line"><span>北京 vs 东京: 0.836</span></span>
<span class="line"><span>北京 vs 苹果: 0.173</span></span>
<span class="line"><span>Python vs Java: 0.450</span></span>
<span class="line"><span>Python vs 香蕉: 0.072</span></span>
<span class="line"><span></span></span></code></pre>
<hr>
<h2 id="四类比在向量空间中如何体现">四、类比在向量空间中如何体现</h2>
<p>Embedding 不仅能表示“相似”，还能隐式表达<strong>关系本身</strong>。在高维向量空间中，<strong>两个词向量的差值，并不是随机噪声，而往往对应一种稳定的语义方向</strong>。</p>
<p>经典例子是：<code>国王 - 男人 + 女人 ≈ 女王</code></p>
<p>这并不是模型“会算术”，而是因为 <code>国王 - 男人</code> 抽取出了“<strong>王权但不含性别</strong>”的语义方向，再加上 <code>女人</code>，就把性别维度切换为女性，得到的向量自然靠近 <code>女王</code>。因此可以说明，关系本身可以被表示为向量方向，某些语义关系在不同词之间是<strong>可迁移、可复用的</strong>。</p>
<p><em>(注：此处 Notebook 中包含一段关于词向量类比的视频展示代码)</em></p>
<hr>
<h2 id="实战-类比">实战: 类比</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 假设这是从预训练模型中提取的 4 维 Embedding 向量（实际模型通常是 768 或 1536 维）</span></span>
<span class="line"><span style="color:#6A737D"># 每一维可能隐含代表：[权力, 性别(男为正,女为负), 生物性, ... ]</span></span>
<span class="line"><span style="color:#E1E4E8">embeddings </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#9ECBFF">    "king"</span><span style="color:#E1E4E8">:   np.array([</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">0.8</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "man"</span><span style="color:#E1E4E8">:    np.array([</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "woman"</span><span style="color:#E1E4E8">:  np.array([</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "queen"</span><span style="color:#E1E4E8">:  np.array([</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#F97583">-</span><span style="color:#79B8FF">0.8</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "apple"</span><span style="color:#E1E4E8">:  np.array([</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># 干扰项</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> find_closest</span><span style="color:#E1E4E8">(target_vec, word_dict):</span></span>
<span class="line"><span style="color:#E1E4E8">    best_word </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> None</span></span>
<span class="line"><span style="color:#E1E4E8">    max_sim </span><span style="color:#F97583">=</span><span style="color:#F97583"> -</span><span style="color:#79B8FF">1</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> word, vec </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> word_dict.items():</span></span>
<span class="line"><span style="color:#E1E4E8">        sim </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.dot(target_vec, vec) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (np.linalg.norm(target_vec) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> np.linalg.norm(vec))</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> sim </span><span style="color:#F97583">></span><span style="color:#E1E4E8"> max_sim:</span></span>
<span class="line"><span style="color:#E1E4E8">            max_sim </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sim</span></span>
<span class="line"><span style="color:#E1E4E8">            best_word </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> word</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> best_word, max_sim</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 1. 执行向量运算：国王 - 男人 + 女人</span></span>
<span class="line"><span style="color:#E1E4E8">result_vec </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> embeddings[</span><span style="color:#9ECBFF">"king"</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> embeddings[</span><span style="color:#9ECBFF">"man"</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> embeddings[</span><span style="color:#9ECBFF">"woman"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. 在空间中寻找离结果最近的词</span></span>
<span class="line"><span style="color:#E1E4E8">closest_word, similarity </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> find_closest(result_vec, embeddings)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"运算结果向量指向的词是: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">closest_word</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"相似度得分: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">similarity</span><span style="color:#F97583">:.4f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>输出结果：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>运算结果向量指向的词是: queen</span></span>
<span class="line"><span>相似度得分: 0.9947</span></span>
<span class="line"><span></span></span></code></pre>
<hr>
<h2 id="五联想是如何发生的">五、联想是如何发生的</h2>
<p>当你向 LLM 输入一个词或一句话时，模型并不会像人类一样“联想到相关事物”，而是输入会把模型的注意力和概率分布推向某个高密度的语义区域。</p>
<p>例如，当你输入 <code>医院</code>，在向量空间中，与“医院”语义最接近、共现频率最高的 Token 会被优先激活，因此模型更容易继续生成：</p>
<ul>
<li>医生</li>
<li>病人</li>
<li>手术</li>
<li>治疗</li>
</ul>
<p>这看起来像是“联想”，但本质上并不是主动思考，而是在一个已经被大量语料反复强化的高密度区域中继续采样下一个 Token。换句话说，模型并不是在“想到医生”，而是在统计意义上，<strong>“医生”是此刻概率最高的延续结果</strong>。</p>
<hr>
<h2 id="实战-联想">实战: 联想</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 1. 定义词表与简化的 Embedding 向量</span></span>
<span class="line"><span style="color:#6A737D"># 每一维可能代表 [生命健康, 科技, 生活琐事]</span></span>
<span class="line"><span style="color:#E1E4E8">vocab </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {</span></span>
<span class="line"><span style="color:#9ECBFF">    "医院"</span><span style="color:#E1E4E8">: np.array([</span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.2</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "医生"</span><span style="color:#E1E4E8">: np.array([</span><span style="color:#79B8FF">0.85</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.3</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "病人"</span><span style="color:#E1E4E8">: np.array([</span><span style="color:#79B8FF">0.8</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.4</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "手术"</span><span style="color:#E1E4E8">: np.array([</span><span style="color:#79B8FF">0.95</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.3</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "代码"</span><span style="color:#E1E4E8">: np.array([</span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.9</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#9ECBFF">    "咖啡"</span><span style="color:#E1E4E8">: np.array([</span><span style="color:#79B8FF">0.2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.8</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> simulate_association</span><span style="color:#E1E4E8">(input_word):</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> input_word </span><span style="color:#F97583">not</span><span style="color:#F97583"> in</span><span style="color:#E1E4E8"> vocab:</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#9ECBFF"> "词不在词表中"</span></span>
<span class="line"><span style="color:#E1E4E8">    </span></span>
<span class="line"><span style="color:#E1E4E8">    input_vec </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> vocab[input_word]</span></span>
<span class="line"><span style="color:#E1E4E8">    probabilities </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # 2. 计算输入词与词表中所有词的相似度（激活强度）</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> word, vec </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> vocab.items():</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> word </span><span style="color:#F97583">==</span><span style="color:#E1E4E8"> input_word: </span><span style="color:#F97583">continue</span></span>
<span class="line"><span style="color:#E1E4E8">        similarity </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.dot(input_vec, vec) </span><span style="color:#F97583">/</span><span style="color:#E1E4E8"> (np.linalg.norm(input_vec) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> np.linalg.norm(vec))</span></span>
<span class="line"><span style="color:#6A737D">        # 将相似度转化为激活概率（简化版的 Softmax）</span></span>
<span class="line"><span style="color:#E1E4E8">        probabilities[word] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.exp(similarity </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> 10</span><span style="color:#E1E4E8">) </span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # 3. 归一化概率</span></span>
<span class="line"><span style="color:#E1E4E8">    total_prob </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> sum</span><span style="color:#E1E4E8">(probabilities.values())</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> word </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> probabilities:</span></span>
<span class="line"><span style="color:#E1E4E8">        probabilities[word] </span><span style="color:#F97583">/=</span><span style="color:#E1E4E8"> total_prob</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">    sorted_probs </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> sorted</span><span style="color:#E1E4E8">(probabilities.items(), </span><span style="color:#FFAB70">key</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8"> x: x[</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], </span><span style="color:#FFAB70">reverse</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> sorted_probs</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 模拟输入“医院”</span></span>
<span class="line"><span style="color:#E1E4E8">associations </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> simulate_association(</span><span style="color:#9ECBFF">"医院"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"--- 输入词：医院 ---"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"模型后续 Token 的激活概率分布："</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> word, prob </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> associations:</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Token: [</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">word</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">] -> 出现概率: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">prob</span><span style="color:#F97583">:.4f</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>输出结果：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>--- 输入词：医院 ---</span></span>
<span class="line"><span>模型后续 Token 的激活概率分布：</span></span>
<span class="line"><span>Token: [医生] -> 出现概率: 0.3717</span></span>
<span class="line"><span>Token: [手术] -> 出现概率: 0.3290</span></span>
<span class="line"><span>Token: [病人] -> 出现概率: 0.2972</span></span>
<span class="line"><span>Token: [咖啡] -> 出现概率: 0.0018</span></span>
<span class="line"><span>Token: [代码] -> 出现概率: 0.0002</span></span>
<span class="line"><span></span></span></code></pre>
<p><a href="https://github.com/flingjie/Agent-100-Days/blob/main/week1/02.Token%E3%80%81Embedding%E4%B8%8E%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4.ipynb">原文链接 Agent-100-Days Token、Embedding 与向量空间</a></p>  </div> <div class="tag-container"> <a href="/tags/大模型学习" class="tag" title="大模型学习"> 大模型学习 </a><a href="/tags/大模型" class="tag" title="大模型"> 大模型 </a> </div> </div>   </div> </div> </div>  </body></html> 